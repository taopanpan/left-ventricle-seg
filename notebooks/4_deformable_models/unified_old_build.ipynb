{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import menpo.io as mio\n",
    "from menpo.landmark import labeller, left_ventricle_34,left_ventricle_34_trimesh,left_ventricle_34_trimesh1\n",
    "\n",
    "from menpo.transform import ThinPlateSplines\n",
    "from menpo.feature import *\n",
    "from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n",
    "from menpofit.fitter import noisy_shape_from_bounding_box\n",
    "from menpofit.visualize import plot_cumulative_error_distribution\n",
    "from menpowidgets import *\n",
    "from menpodetect import load_dlib_left_ventricle_detector\n",
    "from menpo.io import export_pickle,import_pickle\n",
    "from menpo.visualize import print_progress\n",
    "import menpo.visualize\n",
    "\n",
    "import project.utils.labeller_lv as labels\n",
    "from project.utils import tfrecords\n",
    "import predict_lv as predict\n",
    "import project.utils.visualisation as visualisation\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io as io\n",
    "from io import BytesIO\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# method to load a database\n",
    "def load_database(path_to_images, crop_percentage,max_diagonal=400, max_images=None):\n",
    "    images = []\n",
    "    # load landmarked images\n",
    "    for i in mio.import_images(path_to_images, max_images=max_images, verbose=True):\n",
    "           \n",
    "        # convert it to grayscale if needed\n",
    "        if i.n_channels == 3:\n",
    "            i = i.as_greyscale(mode='luminosity')\n",
    "        # crop image\n",
    "        i = i.crop_to_landmarks_proportion(crop_percentage)\n",
    "        \n",
    "        d = i.diagonal()\n",
    "        if d > max_diagonal:\n",
    "            i = i.rescale(float(max_diagonal) / d)\n",
    "        # define a TriMesh which will be useful for Piecewise Affine Warp of HolisticAAM\n",
    "        \n",
    "        labeller(i, 'PTS', left_ventricle_34)#lv_34_trimesh\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh1)\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh)\n",
    "        #i= i.resize([256, 256])\n",
    "        # append it to the list\n",
    "        images.append(i)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020B7B407208>, '_evaluation_master': '', '_model_dir': 'models\\\\lv\\\\lv_1hg_lr1e-3_decay10', '_master': '', '_is_chief': True, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_environment': 'local', '_task_type': None, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_task_id': 0, '_num_worker_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "# hg 特征\n",
    "from project.hourglass import estimator\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from menpo.feature import *\n",
    "import project.hourglass.params as hgparams\n",
    "from tensorflow.python.estimator.inputs import numpy_io \n",
    "\n",
    "from tensorflow.python.training import coordinator\n",
    "from tensorflow.python.training import queue_runner_impl\n",
    "\n",
    "from skimage import transform \n",
    "params = {\n",
    "    hgparams.N_FEATURES: 128,\n",
    "    hgparams.N_HOURGLASS: 1,\n",
    "    hgparams.N_RESIDUALS: 3,\n",
    "}\n",
    "# Where is the model located?\n",
    "model_dir = Path('models/lv/lv_1hg_lr1e-3_decay10/')\n",
    "params[hgparams.N_LANDMARKS] = 34\n",
    "# Instantiate Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    gpu_memory_fraction=1, session_config=config)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        nn = learn.Estimator(\n",
    "        model_dir=str(model_dir),\n",
    "        params=params,\n",
    "        config=run_config,\n",
    "        model_fn=estimator._model_fn)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "@ndfeature\n",
    "def hg(image):\n",
    "    _,h,w= image.shape\n",
    "    image = image.reshape([w,h])\n",
    "    image=transform.resize(image, (256, 256))\n",
    "    image = image.reshape([1,256,256])\n",
    "    data = np.zeros((1, 256, 256, 3), np.float32)  \n",
    "    data[:,:, :, 0] = image\n",
    "    data[:,:, :, 1] = image\n",
    "    data[:,:, :, 2] = image\n",
    "    data=data.astype(np.float32)\n",
    "    #image=image/255.0\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "          x={\"image\": data},\n",
    "          num_epochs=1,\n",
    "          shuffle=False)\n",
    "        predictions = nn.predict(input_fn=input_fn)\n",
    "        images_generator = visualisation.lv_predictions(predictions,\n",
    "                                                    show_input_images=False,\n",
    "                                                    show_combined_heatmap=True,\n",
    "                                                    show_individual_heatmaps=False)\n",
    "\n",
    "    images = menpo.base.LazyList.init_from_iterable(images_generator)\n",
    "\n",
    "    def flatten(list_of_lists): return [\n",
    "        item for sublist in list_of_lists for item in sublist]\n",
    "    images = flatten(images)\n",
    "    images[0] =images[0].resize([w, h]).rescale_pixels(0, 255.0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    return images[0].pixels\n",
    "def hog_closure(image):\n",
    "    return hog(image,cell_size=4, window_step_horizontal=1, window_step_vertical=1)\n",
    "\n",
    "#hg(train_images[1])\n",
    "#type(train_images[1])\n",
    "#hg(train_images[4])\n",
    "#visualize_images(hg(train_images[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 assets, index the returned LazyList to import.\n"
     ]
    }
   ],
   "source": [
    "max_images = 242\n",
    "path_to_images = 'k:/datasets/lv_pts_216/trainset' #/media/taopan/data/\n",
    "train_images = load_database(path_to_images,crop_percentage=0.5,max_images=max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group = 'lv_34'\n",
    "\n",
    "features = no_op\n",
    "patch_normalisation = normalize\n",
    "patch_shape = (17, 17)\n",
    "diagonal = None\n",
    "scales = (1,0.5)\n",
    "offsets = np.meshgrid(range(-4, 5, 2), range(-4, 5, 2))\n",
    "offsets = np.asarray([offsets[0].flatten(), offsets[1].flatten()]).T \n",
    "#offsets = None\n",
    "covariance = 2\n",
    "max_shape_components = 25\n",
    "max_appearance_components = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Building modelsges size: 100%                                                 \n",
      "  - Level 0: Building shape model                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\develop\\menpofit-py3\\menpofit\\builder.py:338: MenpoFitModelBuilderWarning: The reference shape passed is not a TriMesh or subclass and therefore the reference frame (mask) will be calculated via a Delaunay triangulation. This may cause small triangles and thus suboptimal warps.\n",
      "  MenpoFitModelBuilderWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Level 0: Doneding classifiers - 100%                                        \n",
      "  - Level 1: Doneding classifiers - 100%                                        \n",
      "                                                              "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'I:\\\\menpo\\\\project_lv\\\\data\\\\deformable\\\\unified\\\\no-op-unified.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-26d95f4e314b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m                                                                     \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                                                     verbose=True)\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munified\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munified_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#print(unified)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\develop\\menpo-py3\\menpo\\io\\output\\base.py\u001b[0m in \u001b[0;36mexport_pickle\u001b[1;34m(obj, fp, overwrite, protocol)\u001b[0m\n\u001b[0;32m    202\u001b[0m                                                   pickle_types)\n\u001b[0;32m    203\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip_open\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.gz'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[1;31m# force overwrite as True we've already done the check above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             _export(obj, f, pickle_types, extension, True,\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'I:\\\\menpo\\\\project_lv\\\\data\\\\deformable\\\\unified\\\\no-op-unified.pkl'"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,'I:/menpo/project_lv' )\n",
    "from alabortcvpr2015.clm import CLMBuilder\n",
    "from alabortcvpr2015.unified import GlobalUnifiedBuilder\n",
    "from alabortcvpr2015.clm.classifier import  MCF, MultipleMCF, LinearSVMLR, MultipleLinearSVMLR\n",
    "\n",
    "unified_path = Path('I:/menpo/project_lv/data/deformable/unified/no-op-unified.pkl')\n",
    "\n",
    "if unified_path.exists():\n",
    "    unified = mio.import_pickle(unified_path)\n",
    "    print('load done')\n",
    "else:\n",
    "    unified = GlobalUnifiedBuilder(features=features,\n",
    "                               diagonal=diagonal,\n",
    "                               normalize_parts=True,\n",
    "                               covariance=3,\n",
    "                               scales=(1, .5),\n",
    "                               max_shape_components=max_shape_components,\n",
    "                               max_appearance_components=max_appearance_components ).build(train_images,\n",
    "                                                                    group=group,\n",
    "                                                                    verbose=True)\n",
    "    mio.export_pickle(unified,unified_path, overwrite=True, protocol=2)\n",
    "    print('train done')\n",
    "#print(unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Building modelsges size: 100%                                                 \n",
      "  - Level 0: Doneding classifiers - 100%                                        \n",
      "  - Level 1: Doneding classifiers - 100%                                        \n",
      "                                                              train done\n"
     ]
    }
   ],
   "source": [
    "from alabortcvpr2015.unified import PartsUnifiedBuilder\n",
    "unified_path = Path('I:/menpo/project_lv/data/deformable/unified/no-op-patch-unified.pkl')\n",
    "\n",
    "if unified_path.exists():\n",
    "    unified = mio.import_pickle(unified_path)\n",
    "    print('load done')\n",
    "else:\n",
    "    unified = PartsUnifiedBuilder(features=features,\n",
    "                               diagonal=diagonal,\n",
    "                               normalize_parts=True,\n",
    "                               covariance=3,\n",
    "                               scales=(1, .5),\n",
    "                               parts_shape=(17, 17),\n",
    "                               max_shape_components=max_shape_components,\n",
    "                               max_appearance_components=max_appearance_components ).build(train_images,\n",
    "                                                                    group=group,\n",
    "                                                                    verbose=True)\n",
    "    mio.export_pickle(unified,unified_path, overwrite=True, protocol=2)\n",
    "    print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from menpo.image import Image\n",
    "#Image(np.real(np.fft.fftshift(np.fft.ifft2(clm.classifiers[0][36].f)))).view()\n",
    "#clm.view_expert_ensemble_widget()\n",
    "#clm.view_shape_models_widget()\n",
    "#clm.view_clm_widget()\n",
    "#unified.parts_filters()[0][18].view()\n",
    "#Image(unified.appearance_models[0].mean().pixels[18, 0]).view()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import menpo.io as mio\n",
    "from menpo.landmark import labeller, left_ventricle_34,left_ventricle_34_trimesh,left_ventricle_34_trimesh1\n",
    "\n",
    "from menpo.transform import ThinPlateSplines\n",
    "from menpo.feature import *\n",
    "from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n",
    "from menpofit.fitter import noisy_shape_from_bounding_box\n",
    "from menpofit.visualize import plot_cumulative_error_distribution\n",
    "from menpowidgets import *\n",
    "from menpodetect import load_dlib_left_ventricle_detector\n",
    "from menpo.io import export_pickle,import_pickle\n",
    "from menpo.visualize import print_progress\n",
    "import menpo.visualize\n",
    "\n",
    "import project.utils.labeller_lv as labels\n",
    "from project.utils import tfrecords\n",
    "import predict_lv as predict\n",
    "import project.utils.visualisation as visualisation\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io as io\n",
    "from io import BytesIO\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# method to load a database\n",
    "def load_database(path_to_images, crop_percentage,max_diagonal=400, max_images=None):\n",
    "    images = []\n",
    "    # load landmarked images\n",
    "    for i in mio.import_images(path_to_images, max_images=max_images, verbose=True):\n",
    "           \n",
    "        # convert it to grayscale if needed\n",
    "        if i.n_channels == 3:\n",
    "            i = i.as_greyscale(mode='luminosity')\n",
    "        # crop image\n",
    "        i = i.crop_to_landmarks_proportion(crop_percentage)\n",
    "        \n",
    "        d = i.diagonal()\n",
    "        if d > max_diagonal:\n",
    "            i = i.rescale(float(max_diagonal) / d)\n",
    "        # define a TriMesh which will be useful for Piecewise Affine Warp of HolisticAAM\n",
    "        \n",
    "        labeller(i, 'PTS', left_ventricle_34)#lv_34_trimesh\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh1)\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh)\n",
    "        #i= i.resize([256, 256])\n",
    "        # append it to the list\n",
    "        images.append(i)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_is_chief': True, '_save_checkpoints_secs': 600, '_num_worker_replicas': 0, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_save_checkpoints_steps': None, '_environment': 'local', '_tf_random_seed': None, '_master': '', '_task_id': 0, '_num_ps_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_model_dir': 'models\\\\lv\\\\lv_1hg_lr1e-3_decay10', '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000241C43783C8>}\n"
     ]
    }
   ],
   "source": [
    "# hg 特征\n",
    "from project.hourglass import estimator\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from menpo.feature import *\n",
    "import project.hourglass.params as hgparams\n",
    "from tensorflow.python.estimator.inputs import numpy_io \n",
    "\n",
    "from tensorflow.python.training import coordinator\n",
    "from tensorflow.python.training import queue_runner_impl\n",
    "\n",
    "from skimage import transform \n",
    "params = {\n",
    "    hgparams.N_FEATURES: 128,\n",
    "    hgparams.N_HOURGLASS: 1,\n",
    "    hgparams.N_RESIDUALS: 3,\n",
    "}\n",
    "# Where is the model located?\n",
    "model_dir = Path('models/lv/lv_1hg_lr1e-3_decay10/')\n",
    "params[hgparams.N_LANDMARKS] = 34\n",
    "# Instantiate Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    gpu_memory_fraction=1, session_config=config)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        nn = learn.Estimator(\n",
    "        model_dir=str(model_dir),\n",
    "        params=params,\n",
    "        config=run_config,\n",
    "        model_fn=estimator._model_fn)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "@ndfeature\n",
    "def hg(image):\n",
    "    _,h,w= image.shape\n",
    "    image = image.reshape([w,h])\n",
    "    image=transform.resize(image, (256, 256))\n",
    "    image = image.reshape([1,256,256])\n",
    "    data = np.zeros((1, 256, 256, 3), np.float32)  \n",
    "    data[:,:, :, 0] = image\n",
    "    data[:,:, :, 1] = image\n",
    "    data[:,:, :, 2] = image\n",
    "    data=data.astype(np.float32)\n",
    "    #image=image/255.0\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "          x={\"image\": data},\n",
    "          num_epochs=1,\n",
    "          shuffle=False)\n",
    "        predictions = nn.predict(input_fn=input_fn)\n",
    "        images_generator = visualisation.lv_predictions(predictions,\n",
    "                                                    show_input_images=False,\n",
    "                                                    show_combined_heatmap=True,\n",
    "                                                    show_individual_heatmaps=False)\n",
    "\n",
    "    images = menpo.base.LazyList.init_from_iterable(images_generator)\n",
    "\n",
    "    def flatten(list_of_lists): return [\n",
    "        item for sublist in list_of_lists for item in sublist]\n",
    "    images = flatten(images)\n",
    "    images[0] =images[0].resize([w, h]).rescale_pixels(0, 255.0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    return images[0].pixels\n",
    "def hog_closure(image):\n",
    "    return hog(image,cell_size=4, window_step_horizontal=1, window_step_vertical=1)\n",
    "\n",
    "#hg(train_images[1])\n",
    "#type(train_images[1])\n",
    "#hg(train_images[4])\n",
    "#visualize_images(hg(train_images[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 assets, index the returned LazyList to import.\n"
     ]
    }
   ],
   "source": [
    "max_images = 242\n",
    "path_to_images = 'k:/datasets/lv_pts_216/trainset' #/media/taopan/data/\n",
    "train_images = load_database(path_to_images,crop_percentage=0.5,max_images=max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'lv_34'\n",
    "\n",
    "features = no_op\n",
    "patch_normalisation = normalize\n",
    "patch_shape = (17, 17)\n",
    "diagonal = None\n",
    "scales = (1,0.5)\n",
    "offsets = np.meshgrid(range(-4, 5, 2), range(-4, 5, 2))\n",
    "offsets = np.asarray([offsets[0].flatten(), offsets[1].flatten()]).T \n",
    "#offsets = None\n",
    "covariance = 2\n",
    "max_shape_components = 25\n",
    "max_appearance_components = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Building modelsges size: 100%                                                 \n",
      "  - Level 0: Computing feature space: 89%                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8e30d2bb7e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                  \u001b[0mcovariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                  \u001b[0mscales\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscales\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_shape_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_shape_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                  offsets=offsets).build(train_images, group=group,  verbose=True)\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclm_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:/menpo/project_lv\\alabortcvpr2015\\clm\\builder.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, images, group, label, verbose, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;31m# compute features at highest level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 feature_images = self._compute_features(images, level_str,\n\u001b[1;32m---> 64\u001b[1;33m                                                         verbose)\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mlevel_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:/menpo/project_lv\\alabortcvpr2015\\clm\\builder.py\u001b[0m in \u001b[0;36m_compute_features\u001b[1;34m(self, images, level_str, verbose)\u001b[0m\n\u001b[0;32m    178\u001b[0m                                          show_bar=False)))\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[0mfeature_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\develop\\menpo-py3\\menpo\\feature\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(image, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;31m# Image supplied to ndarray feature -\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;31m# extract pixels and go\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrebuild_feature_image_with_centres\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\develop\\menpo-py3\\menpo\\feature\\features.py\u001b[0m in \u001b[0;36mhog\u001b[1;34m(pixels, mode, algorithm, num_bins, cell_size, block_size, signed_gradient, l2_norm_clip, window_height, window_width, window_unit, window_step_vertical, window_step_horizontal, window_step_unit, padding, verbose)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;31m# Compute HOG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     hog_descriptor = iterator.HOG(algorithm, num_bins, cell_size, block_size,\n\u001b[1;32m--> 320\u001b[1;33m                                   signed_gradient, l2_norm_clip, verbose)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# TODO: This is a temporal fix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# flip axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\develop\\menpo-py3\\menpo\\feature\\windowiterator.pyx\u001b[0m in \u001b[0;36mmenpo.feature.windowiterator.WindowIterator.HOG\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mascontiguousarray\u001b[1;34m(a, dtype)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \"\"\"\n\u001b[1;32m--> 569\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masfortranarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,'I:/menpo/project_lv' )\n",
    "from alabortcvpr2015.clm import CLMBuilder\n",
    "from alabortcvpr2015.clm.classifier import  MCF, MultipleMCF, LinearSVMLR, MultipleLinearSVMLR\n",
    "\n",
    "clm_path = Path('I:/menpo/project_lv/data/deformable/clm/hog-mcf-clm.pkl')\n",
    "\n",
    "if clm_path.exists():\n",
    "    clm= mio.import_pickle(clm_path)\n",
    "    print('load done')\n",
    "else:\n",
    "    clm = CLMBuilder(parts_shape=(17, 17),\n",
    "                 features=features,\n",
    "                 diagonal=diagonal,\n",
    "                 classifier=MCF,\n",
    "                 normalize_parts=True,\n",
    "                 covariance=covariance,\n",
    "                 scales=scales,max_shape_components=max_shape_components,\n",
    "                 offsets=offsets).build(train_images, group=group,  verbose=True)\n",
    "    mio.export_pickle(clm,clm_path, overwrite=True, protocol=2)\n",
    "    print('train done')\n",
    "#print(clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_path = Path('I:/menpo/project_lv/data/deformable/clm/hog-svmlr-clm.pkl')\n",
    "\n",
    "if clm_path.exists():\n",
    "    clm= mio.import_pickle(clm_path)\n",
    "    print('load done')\n",
    "else:\n",
    "    clm = CLMBuilder(parts_shape=(17, 17),\n",
    "                 features=features,\n",
    "                 diagonal=diagonal,\n",
    "                 classifier=LinearSVMLR,\n",
    "                 normalize_parts=True,\n",
    "                 covariance=covariance,\n",
    "                 scales=scales,max_shape_components=max_shape_components,\n",
    "                 offsets=offsets).build(train_images, group=group,  verbose=True)\n",
    "    mio.export_pickle(clm,clm_path, overwrite=True, protocol=2)\n",
    "    print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from menpo.image import Image\n",
    "#Image(np.real(np.fft.fftshift(np.fft.ifft2(clm.classifiers[0][36].f)))).view()\n",
    "#clm.view_expert_ensemble_widget()\n",
    "#clm.view_shape_models_widget()\n",
    "#clm.view_clm_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

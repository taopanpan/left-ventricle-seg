{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io as io\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate_lv as evaluate\n",
    "\n",
    "from project.utils import labeller_lv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Evaluation\n",
    "\n",
    "How well do the models perform on your test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where are the models located?\n",
    "Which models do you want to evaluate? Please supply the parent directory of the model directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where are the models located?\n",
    "models_dir = '/media/taopan/data/landmark/00_project-master/models/lv'\n",
    "\n",
    "filename = Path(models_dir) / 'model.ckpt-4668'\n",
    "\n",
    "# Where are the .tfrecords?\n",
    "test_data = '/media/taopan/data/landmark/00_project-master/data/tfrecords/lv/test.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are you ready?\n",
    "Run the next cell to evaluate the model.\n",
    "Evaluation metrics returned in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import project.hourglass.params as hgparams\n",
    "\n",
    "params = {\n",
    "    hgparams.N_FEATURES: 256,\n",
    "    hgparams.N_HOURGLASS: 4,\n",
    "    hgparams.N_RESIDUALS: 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will perform evaluation on the following models in `/media/taopan/data/landmark/00_project-master/models/lv`\n",
      "['lv_4hg_lr1e-3_decay10']\n",
      "()\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88c4e2190>, '_model_dir': '/media/taopan/data/landmark/00_project-master/models/lv/lv_4hg_lr1e-3_decay10', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "DEBUG:tensorflow:Setting feature info to {'image': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(4), Dimension(256), Dimension(256), Dimension(3)]), is_sparse=False), 'scale': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(4)]), is_sparse=False), 'marked_idx': TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(4), Dimension(34)]), is_sparse=False)}.\n",
      "DEBUG:tensorflow:Setting labels info to {'heatmap': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(4), Dimension(256), Dimension(256), Dimension(34)]), is_sparse=False), 'coordinates': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(4), Dimension(34), Dimension(2)]), is_sparse=False)}\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-09-06:58:16\n",
      "INFO:tensorflow:Restoring parameters from /media/taopan/data/landmark/00_project-master/models/lv/lv_4hg_lr1e-3_decay10/model.ckpt-4668\n",
      "INFO:tensorflow:Evaluation [1/53]\n",
      "INFO:tensorflow:Evaluation [2/53]\n",
      "INFO:tensorflow:Evaluation [3/53]\n",
      "INFO:tensorflow:Evaluation [4/53]\n",
      "INFO:tensorflow:Evaluation [5/53]\n",
      "INFO:tensorflow:Evaluation [6/53]\n",
      "INFO:tensorflow:Evaluation [7/53]\n",
      "INFO:tensorflow:Evaluation [8/53]\n",
      "INFO:tensorflow:Evaluation [9/53]\n",
      "INFO:tensorflow:Evaluation [10/53]\n",
      "INFO:tensorflow:Evaluation [11/53]\n",
      "INFO:tensorflow:Evaluation [12/53]\n",
      "INFO:tensorflow:Evaluation [13/53]\n",
      "INFO:tensorflow:Evaluation [14/53]\n",
      "INFO:tensorflow:Evaluation [15/53]\n",
      "INFO:tensorflow:Evaluation [16/53]\n",
      "INFO:tensorflow:Evaluation [17/53]\n",
      "INFO:tensorflow:Evaluation [18/53]\n",
      "INFO:tensorflow:Evaluation [19/53]\n",
      "INFO:tensorflow:Evaluation [20/53]\n",
      "INFO:tensorflow:Evaluation [21/53]\n",
      "INFO:tensorflow:Evaluation [22/53]\n",
      "INFO:tensorflow:Evaluation [23/53]\n",
      "INFO:tensorflow:Evaluation [24/53]\n",
      "INFO:tensorflow:Evaluation [25/53]\n",
      "INFO:tensorflow:Evaluation [26/53]\n",
      "INFO:tensorflow:Evaluation [27/53]\n",
      "INFO:tensorflow:Evaluation [28/53]\n",
      "INFO:tensorflow:Evaluation [29/53]\n",
      "INFO:tensorflow:Evaluation [30/53]\n",
      "INFO:tensorflow:Evaluation [31/53]\n",
      "INFO:tensorflow:Evaluation [32/53]\n",
      "INFO:tensorflow:Evaluation [33/53]\n",
      "INFO:tensorflow:Evaluation [34/53]\n",
      "INFO:tensorflow:Evaluation [35/53]\n",
      "INFO:tensorflow:Evaluation [36/53]\n",
      "INFO:tensorflow:Evaluation [37/53]\n",
      "INFO:tensorflow:Evaluation [38/53]\n",
      "INFO:tensorflow:Evaluation [39/53]\n",
      "INFO:tensorflow:Evaluation [40/53]\n",
      "INFO:tensorflow:Evaluation [41/53]\n",
      "INFO:tensorflow:Evaluation [42/53]\n",
      "INFO:tensorflow:Evaluation [43/53]\n",
      "INFO:tensorflow:Evaluation [44/53]\n",
      "INFO:tensorflow:Evaluation [45/53]\n",
      "INFO:tensorflow:Evaluation [46/53]\n",
      "INFO:tensorflow:Evaluation [47/53]\n",
      "INFO:tensorflow:Evaluation [48/53]\n",
      "INFO:tensorflow:Evaluation [49/53]\n",
      "INFO:tensorflow:Evaluation [50/53]\n",
      "INFO:tensorflow:Evaluation [51/53]\n",
      "INFO:tensorflow:Evaluation [52/53]\n",
      "INFO:tensorflow:Evaluation [53/53]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-09-06:59:03\n",
      "INFO:tensorflow:Saving dict for global step 4668: global_step = 4668, loss = 0.00212945, normalised_mean_error/all = 45.4765, normalised_mean_error/in = 38.0465, normalised_mean_error/out = 40.9815, pck/all = 0.0, pck/in = 0.0147059, pck/out = 0.0\n",
      "===========lv_4hg_lr1e-3_decay10========\n",
      "normalised_mean_error/in = 38.0465011597\n",
      "global_step = 4668\n",
      "loss = 0.00212945230305\n",
      "normalised_mean_error/all = 45.4765396118\n",
      "pck/in = 0.0147058824077\n",
      "normalised_mean_error/out = 40.9815177917\n",
      "pck/out = 0.0\n",
      "pck/all = 0.0\n"
     ]
    }
   ],
   "source": [
    "def get_immediate_subdirectories(models_dir):\n",
    "    return [name for name in os.listdir(models_dir)\n",
    "            if os.path.isdir(os.path.join(models_dir, name))]\n",
    "\n",
    "\n",
    "print('Will perform evaluation on the following models in `{}`'.format(models_dir))\n",
    "print(get_immediate_subdirectories(models_dir))\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_for_everybody = {}\n",
    "\n",
    "for name in get_immediate_subdirectories(models_dir):\n",
    "    model_dir = os.path.join(models_dir, name)\n",
    "    metrics = evaluate.evaluate(model_dir, test_data, params)\n",
    "    metrics_for_everybody[name] = metrics\n",
    "    \n",
    "for model_dir, metrics in metrics_for_everybody.items():\n",
    "    print('==========={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results are in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===lv_4hg_lr1e-3_decay10========\n",
      "normalised_mean_error/in = 38.0465011597 <type is `<type 'numpy.float32'>`>\n",
      "global_step = 4668 <type is `<type 'numpy.int64'>`>\n",
      "loss = 0.00212945230305 <type is `<type 'numpy.float32'>`>\n",
      "normalised_mean_error/all = 45.4765396118 <type is `<type 'numpy.float32'>`>\n",
      "pck/in = 0.0147058824077 <type is `<type 'numpy.float32'>`>\n",
      "normalised_mean_error/out = 40.9815177917 <type is `<type 'numpy.float32'>`>\n",
      "pck/out = 0.0 <type is `<type 'numpy.float32'>`>\n",
      "pck/all = 0.0 <type is `<type 'numpy.float32'>`>\n"
     ]
    }
   ],
   "source": [
    "for model_dir, metrics in metrics_for_everybody.items():\n",
    "    print('==={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {} <type is `{}`>'.format(metric, value, type(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# class MyEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, np.integer):\n",
    "#             return int(obj)\n",
    "#         elif isinstance(obj, np.floating):\n",
    "#             return float(obj)\n",
    "#         elif isinstance(obj, np.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         else:\n",
    "#             return super(MyEncoder, self).default(obj)\n",
    "        \n",
    "# def write_json(filename, model_metrics):\n",
    "#     print('Writing JSON results to {}'.format(filename))\n",
    "#     with open(filename, 'w') as handle:\n",
    "#       json.dump(metrics_for_everybody, handle, sort_keys=True, indent=4, cls=MyEncoder)\n",
    "    \n",
    "# if metrics_for_everybody:\n",
    "#     write_json('{}.json'.format(filename), metrics_for_everybody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/taopan/data/landmark/00_project-master\n",
      "Writing CSV results to /media/taopan/data/landmark/00_project-master/models/lv/model.ckpt-4668.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_csv(filename, model_metrics):\n",
    "    print('Writing CSV results to {}'.format(filename))\n",
    "    df = pd.DataFrame(metrics_for_everybody)\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "if metrics_for_everybody:\n",
    "    \n",
    "    print(os.getcwd())\n",
    "    \n",
    "    write_csv('{}.csv'.format(filename), metrics_for_everybody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========lv_4hg_lr1e-3_decay10========\n",
      "loss = 0.00212945230305\n",
      "global_step = 4668.0\n",
      "normalised_mean_error/out = 40.9815177917\n",
      "normalised_mean_error/all = 45.4765396118\n",
      "pck/in = 0.0147058824077\n",
      "pck/out = 0.0\n",
      "pck/all = 0.0\n",
      "normalised_mean_error/in = 38.0465011597\n"
     ]
    }
   ],
   "source": [
    "def csv_to_dict(filename):\n",
    "    dic = pd.DataFrame.from_csv(filename).to_dict()\n",
    "    return dic\n",
    "\n",
    "dic = csv_to_dict('{}.csv'.format(filename))\n",
    "\n",
    "for model_dir, metrics in dic.items():\n",
    "    print('==========={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

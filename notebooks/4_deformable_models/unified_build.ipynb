{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import menpo.io as mio\n",
    "from menpo.landmark import labeller, left_ventricle_34,left_ventricle_34_trimesh,left_ventricle_34_trimesh1\n",
    "\n",
    "from menpo.transform import ThinPlateSplines\n",
    "from menpo.feature import *\n",
    "from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n",
    "from menpofit.fitter import noisy_shape_from_bounding_box\n",
    "from menpofit.visualize import plot_cumulative_error_distribution\n",
    "from menpowidgets import *\n",
    "from menpodetect import load_dlib_left_ventricle_detector\n",
    "from menpo.io import export_pickle,import_pickle\n",
    "from menpo.visualize import print_progress\n",
    "import menpo.visualize\n",
    "\n",
    "import project.utils.labeller_lv as labels\n",
    "from project.utils import tfrecords\n",
    "import predict_lv as predict\n",
    "import project.utils.visualisation as visualisation\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io as io\n",
    "from io import BytesIO\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# method to load a database\n",
    "def load_database(path_to_images, crop_percentage,max_diagonal=400, max_images=None):\n",
    "    images = []\n",
    "    # load landmarked images\n",
    "    for i in mio.import_images(path_to_images, max_images=max_images, verbose=True):\n",
    "           \n",
    "        # convert it to grayscale if needed\n",
    "        if i.n_channels == 3:\n",
    "            i = i.as_greyscale(mode='luminosity')\n",
    "        # crop image\n",
    "        i = i.crop_to_landmarks_proportion(crop_percentage)\n",
    "        \n",
    "        d = i.diagonal()\n",
    "        if d > max_diagonal:\n",
    "            i = i.rescale(float(max_diagonal) / d)\n",
    "        # define a TriMesh which will be useful for Piecewise Affine Warp of HolisticAAM\n",
    "        \n",
    "        labeller(i, 'PTS', left_ventricle_34)#lv_34_trimesh\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh1)\n",
    "        labeller(i, 'PTS', left_ventricle_34_trimesh)\n",
    "        #i= i.resize([256, 256])\n",
    "        # append it to the list\n",
    "        images.append(i)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DF28CD8240>, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_model_dir': 'models\\\\lv\\\\lv_1hg_lr1e-3_decay10', '_task_id': 0, '_environment': 'local', '_task_type': None, '_save_summary_steps': 100, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# hg 特征\n",
    "from project.hourglass import estimator\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from menpo.feature import *\n",
    "import project.hourglass.params as hgparams\n",
    "from tensorflow.python.estimator.inputs import numpy_io \n",
    "\n",
    "from tensorflow.python.training import coordinator\n",
    "from tensorflow.python.training import queue_runner_impl\n",
    "\n",
    "from skimage import transform \n",
    "params = {\n",
    "    hgparams.N_FEATURES: 128,\n",
    "    hgparams.N_HOURGLASS: 1,\n",
    "    hgparams.N_RESIDUALS: 3,\n",
    "}\n",
    "# Where is the model located?\n",
    "model_dir = Path('models/lv/lv_1hg_lr1e-3_decay10/')\n",
    "params[hgparams.N_LANDMARKS] = 34\n",
    "# Instantiate Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    gpu_memory_fraction=1, session_config=config)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        nn = learn.Estimator(\n",
    "        model_dir=str(model_dir),\n",
    "        params=params,\n",
    "        config=run_config,\n",
    "        model_fn=estimator._model_fn)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "@ndfeature\n",
    "def hg(image):\n",
    "    _,h,w= image.shape\n",
    "    image = image.reshape([w,h])\n",
    "    image=transform.resize(image, (256, 256))\n",
    "    image = image.reshape([1,256,256])\n",
    "    data = np.zeros((1, 256, 256, 3), np.float32)  \n",
    "    data[:,:, :, 0] = image\n",
    "    data[:,:, :, 1] = image\n",
    "    data[:,:, :, 2] = image\n",
    "    data=data.astype(np.float32)\n",
    "    #image=image/255.0\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "          x={\"image\": data},\n",
    "          num_epochs=1,\n",
    "          shuffle=False)\n",
    "        predictions = nn.predict(input_fn=input_fn)\n",
    "        images_generator = visualisation.lv_predictions(predictions,\n",
    "                                                    show_input_images=False,\n",
    "                                                    show_combined_heatmap=True,\n",
    "                                                    show_individual_heatmaps=False)\n",
    "\n",
    "    images = menpo.base.LazyList.init_from_iterable(images_generator)\n",
    "\n",
    "    def flatten(list_of_lists): return [\n",
    "        item for sublist in list_of_lists for item in sublist]\n",
    "    images = flatten(images)\n",
    "    images[0] =images[0].resize([w, h]).rescale_pixels(0, 255.0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    return images[0].pixels\n",
    "def hog_closure(image):\n",
    "    return hog(image,cell_size=4, window_step_horizontal=1, window_step_vertical=1)\n",
    "\n",
    "#hg(train_images[1])\n",
    "#type(train_images[1])\n",
    "#hg(train_images[4])\n",
    "#visualize_images(hg(train_images[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 assets, index the returned LazyList to import.\n"
     ]
    }
   ],
   "source": [
    "max_images = 242\n",
    "path_to_images = 'k:/datasets/lv_pts_216/trainset' #/media/taopan/data/\n",
    "train_images = load_database(path_to_images,crop_percentage=0.5,max_images=max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group = 'lv_34'\n",
    "\n",
    "features = no_op\n",
    "patch_normalisation = normalize_norm\n",
    "patch_shape = (17, 17)\n",
    "diagonal = None\n",
    "scales = (1,0.5)\n",
    "offsets = np.meshgrid(range(-4, 5, 2), range(-4, 5, 2))\n",
    "offsets = np.asarray([offsets[0].flatten(), offsets[1].flatten()]).T \n",
    "#offsets = None\n",
    "covariance = 2\n",
    "max_shape_components = 25\n",
    "max_appearance_components = 250\n",
    "@imgfeature\n",
    "def custom_double_igo(image):\n",
    "    return igo(igo(image))\n",
    "#dsift(training_images[0] ).view()\n",
    "@ndfeature\n",
    "def float32_fast_dsift(x):\n",
    "    return fast_dsift(x).as_type(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Normalizing images size: [          ] 6% (14/212) - 00:00:01 remaining        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\develop\\menpo-py3\\menpo\\shape\\pointcloud.py:261: MenpoDeprecationWarning: The .lms property is deprecated. LandmarkGroups are now shapes themselves - so you can use them directly anywhere you previously used .lms.Simply remove \".lms\" from your code and things will work as expected (and this warning will go away)\n",
      "  MenpoDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Building modelsges size: [==========] 100% (212/212) - done.                  \n",
      "  - Scale 0: Building appearance model                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\develop\\menpofit-py3\\menpofit\\builder.py:338: MenpoFitModelBuilderWarning: The reference shape passed is not a TriMesh or subclass and therefore the reference frame (mask) will be calculated via a Delaunay triangulation. This may cause small triangles and thus suboptimal warps.\n",
      "  MenpoFitModelBuilderWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Scale 0: Donening experts: [==========] 100% (34/34) - done.                \n",
      "  - Scale 1: Donening experts: [==========] 100% (34/34) - done.                \n",
      "  - Scale 2: Donening experts: [==========] 100% (34/34) - done.                \n",
      "                                                              train done\n"
     ]
    }
   ],
   "source": [
    "from menpofit.unified_aam_clm import UnifiedAAMCLM\n",
    "\n",
    "unified_path = Path('I:/menpo/project_lv/data/deformable/unified/no-op-unified1.pkl')\n",
    "\n",
    "if unified_path.exists():\n",
    "    unified = mio.import_pickle(unified_path)\n",
    "    print('load done')\n",
    "else:\n",
    "    unified  = UnifiedAAMCLM(train_images, group='lv_34', reference_shape=None,\n",
    "                  diagonal=None, scales=(0.25, 0.5, 1.0), holistic_features=features, verbose=True,\n",
    "                   max_appearance_components=0.9, max_shape_components=20)\n",
    "    mio.export_pickle(unified,unified_path, overwrite=True, protocol=2)\n",
    "    print('train done')\n",
    "#print(unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from menpo.image import Image\n",
    "#Image(np.real(np.fft.fftshift(np.fft.ifft2(clm.classifiers[0][36].f)))).view()\n",
    "#clm.view_expert_ensemble_widget()\n",
    "#clm.view_shape_models_widget()\n",
    "#clm.view_clm_widget()\n",
    "#unified.parts_filters()[0][18].view()\n",
    "#Image(unified.appearance_models[0].mean().pixels[18, 0]).view()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

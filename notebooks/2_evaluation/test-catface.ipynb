{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io as io\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate_catface as evaluate\n",
    "\n",
    "from project.utils import labeller_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Evaluation\n",
    "\n",
    "How well do the models perform on your test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where are the models located?\n",
    "Which models do you want to evaluate? Please supply the parent directory of the model directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where are the models located?\n",
    "models_dir = 'models/catface/2hg/2hg-noAug'\n",
    "\n",
    "filename = Path(models_dir) / 'eval-train80'\n",
    "\n",
    "# Where are the .tfrecords?\n",
    "test_data = 'data/tfrecords/catface/80-10-10/train80.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are you ready?\n",
    "Run the next cell to evaluate the model.\n",
    "Evaluation metrics returned in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import project.hourglass.params as hgparams\n",
    "\n",
    "params = {\n",
    "    hgparams.N_FEATURES: 256,\n",
    "    hgparams.N_HOURGLASS: 2,\n",
    "    hgparams.N_RESIDUALS: 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(models_dir):\n",
    "    return [name for name in os.listdir(models_dir)\n",
    "            if os.path.isdir(os.path.join(models_dir, name))]\n",
    "\n",
    "\n",
    "print('Will perform evaluation on the following models in `{}`'.format(models_dir))\n",
    "print(get_immediate_subdirectories(models_dir))\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_for_everybody = {}\n",
    "\n",
    "for name in get_immediate_subdirectories(models_dir):\n",
    "    model_dir = os.path.join(models_dir, name)\n",
    "    metrics = evaluate.evaluate(model_dir, test_data, params)\n",
    "    metrics_for_everybody[name] = metrics\n",
    "    \n",
    "for model_dir, metrics in metrics_for_everybody.items():\n",
    "    print('==========={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results are in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir, metrics in metrics_for_everybody.items():\n",
    "    print('==={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {} <type is `{}`>'.format(metric, value, type(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# class MyEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, np.integer):\n",
    "#             return int(obj)\n",
    "#         elif isinstance(obj, np.floating):\n",
    "#             return float(obj)\n",
    "#         elif isinstance(obj, np.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         else:\n",
    "#             return super(MyEncoder, self).default(obj)\n",
    "        \n",
    "# def write_json(filename, model_metrics):\n",
    "#     print('Writing JSON results to {}'.format(filename))\n",
    "#     with open(filename, 'w') as handle:\n",
    "#       json.dump(metrics_for_everybody, handle, sort_keys=True, indent=4, cls=MyEncoder)\n",
    "    \n",
    "# if metrics_for_everybody:\n",
    "#     write_json('{}.json'.format(filename), metrics_for_everybody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_csv(filename, model_metrics):\n",
    "    print('Writing CSV results to {}'.format(filename))\n",
    "    df = pd.DataFrame(metrics_for_everybody)\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "if metrics_for_everybody:\n",
    "    \n",
    "    print(os.getcwd())\n",
    "    \n",
    "    write_csv('{}.csv'.format(filename), metrics_for_everybody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(filename):\n",
    "    dic = pd.DataFrame.from_csv(filename).to_dict()\n",
    "    return dic\n",
    "\n",
    "dic = csv_to_dict('{}.csv'.format(filename))\n",
    "\n",
    "for model_dir, metrics in dic.items():\n",
    "    print('==========={}========'.format(model_dir))\n",
    "    for metric, value in metrics.items():\n",
    "        print('{} = {}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import menpo.io as mio\n",
    "import tensorflow as tf\n",
    "from menpo.visualize import print_progress\n",
    "from project.utils import tfrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate TFRecords files\n",
    "\n",
    "The recommended format for TensorFlow is a TFRecords file containing `tf.train.Example` protocol buffers (which contain `Features` as a field).\n",
    "\n",
    "Here is a little program that gets your data, stuffs it in an `Example` protocol buffer, serializes the protocol buffer to a string, and then writes the string to a TFRecords file using the `tf.python_io.TFRecordWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Functions for writing TFRecord features \"\"\"\n",
    "\n",
    "\n",
    "def _int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def get_jpg_string(image):\n",
    "    # Gets the serialized jpg from a menpo `Image`.\n",
    "    fp = BytesIO()\n",
    "    mio.export_image(image, fp, extension='jpg')\n",
    "    fp.seek(0)\n",
    "    return fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def face_iterator(images):\n",
    "    \"\"\" Given an iterable of images, returns a generator of cat face data \"\"\"\n",
    "    for idx, img in enumerate(print_progress(images, end_with_newline=False)):\n",
    "        image_name = img.path.name\n",
    "\n",
    "        yield image_name, img\n",
    "\n",
    "\n",
    "def generate(iterator,\n",
    "             store_path='./',\n",
    "             record_name='inference.tfrecords',\n",
    "             store_records=True):\n",
    "    store_path = Path(store_path)\n",
    "\n",
    "    if store_records:\n",
    "        writer = tf.python_io.TFRecordWriter(str(store_path / record_name))\n",
    "\n",
    "    for img_name, pimg in iterator:\n",
    "        \n",
    "        # resize image to 256 * 256\n",
    "        cimg = pimg.resize([256, 256])\n",
    "\n",
    "        img_path = store_path / '{}'.format(img_name)\n",
    "\n",
    "        if store_records:\n",
    "            try:\n",
    "                # construct the Example proto object\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(\n",
    "                        # Features contains a map of string to Feature proto objects\n",
    "                        feature={\n",
    "                            # images\n",
    "                            'image': tfrecords.bytes_feature(get_jpg_string(cimg)),\n",
    "                            'height': tfrecords.int_feature(cimg.shape[0]),\n",
    "                            'width': tfrecords.int_feature(cimg.shape[1]),\n",
    "                        }))\n",
    "                # use the proto object to serialize the example to a string\n",
    "                serialized = example.SerializeToString()\n",
    "                # write the serialized object to disk\n",
    "                writer.write(serialized)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Something bad happened when processing image: \"{}\"'.format(img_name))\n",
    "                print(e)\n",
    "\n",
    "    if store_records:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TFRecords for Inference\n",
    "\n",
    "Run the following cells to generate TFRecords for a directory of images to perform inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where are the images located?\n",
    "images_folder = Path('data/images')\n",
    "\n",
    "# where should the resulting TFRecords files be written to?\n",
    "store_path = Path('data/images')\n",
    "inference_record_name = \"inference.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 assets, index the returned LazyList to import.\n",
      "Found 168 assets\n",
      "[====================] 100% (168/168) - done.                                   "
     ]
    }
   ],
   "source": [
    "# Run this to generate the TFRecord file!\n",
    "from menpo.landmark import labeller, left_ventricle_34,left_ventricle_34_trimesh,left_ventricle_34_trimesh1\n",
    "from menpodetect import load_dlib_left_ventricle_detector\n",
    "# load the images\n",
    "#images = mio.import_images(images_folder)\n",
    "detector=load_dlib_left_ventricle_detector(\"detector.svm\")\n",
    "def load_database(path_to_images, crop_percentage,max_diagonal=400, max_images=None):\n",
    "    images = []\n",
    "    # load landmarked images\n",
    "    for i in mio.import_images(path_to_images, max_images=max_images, verbose=True):\n",
    "           \n",
    "        # convert it to grayscale if needed\n",
    "        if i.n_channels == 3:\n",
    "            i = i.as_greyscale(mode='luminosity')\n",
    "        \n",
    "        \n",
    "        d = i.diagonal()\n",
    "        if d > max_diagonal:\n",
    "            i = i.rescale(float(max_diagonal) / d)\n",
    "        bboxes = detector(i)\n",
    "        #print(\"{} detected .\".format(len(bboxes)),len(images),i.path)\n",
    "        initial_bbox = bboxes[0]\n",
    "        # crop image\n",
    "        i = i.crop_to_pointcloud(initial_bbox,boundary=40 )\n",
    "        # append it to the list\n",
    "        images.append(i)\n",
    "    return images\n",
    "crop_percentage = 0.5\n",
    "path_to_lfpw = images_folder\n",
    "images = load_database(path_to_lfpw,crop_percentage)\n",
    "print('Found {} assets'.format(len(images)))\n",
    "\n",
    "# generate TFRecords\n",
    "generate(face_iterator(images), store_path, inference_record_name,\n",
    "         store_records=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
